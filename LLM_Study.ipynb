{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"mount_file_id":"17MLwni5aSeZFVcq3VgoiYHslAcJaOImH","authorship_tag":"ABX9TyPUeGM6J7xRuY0O9UI2G/ij"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# LLM 만들어보기"],"metadata":{"id":"yznIOd0O-Vdl"}},{"cell_type":"markdown","source":["LLM 구축부터 미세튜닝까지의 전 과정을 다음 4단계로 요약하여 진행합니다."],"metadata":{"id":"rs0ZH1aU-dhz"}},{"cell_type":"code","source":["!pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_cbF-5KT968s","executionInfo":{"status":"ok","timestamp":1764338236809,"user_tz":-540,"elapsed":5345,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"c2f5f1f4-735e-4c8a-f776-c347edadd56a"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n"]}]},{"cell_type":"code","source":["from importlib.metadata import version\n","\n","print('파이토치 버전 : ', version('torch'))\n","print('tiktoken version : ' , version('tiktoken'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FP0TSPYo9_Vf","executionInfo":{"status":"ok","timestamp":1764338236943,"user_tz":-540,"elapsed":83,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"df28a84c-2296-445d-a0bc-2cea28972a4b"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["파이토치 버전 :  2.9.0+cu126\n","tiktoken version :  0.12.0\n"]}]},{"cell_type":"code","source":["file_path = \"/content/drive/MyDrive/study/LLM/the-verdict.txt\""],"metadata":{"id":"CiK41_UkTW7j","executionInfo":{"status":"ok","timestamp":1764338236946,"user_tz":-540,"elapsed":2,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["## **텍스트 토큰화하기**"],"metadata":{"id":"0dK4svEx-ZBL"}},{"cell_type":"markdown","source":["* 사용할 텍스트 데이터는 이디스 워튼의 단편 소설 \"The Verdict\" 가져오기"],"metadata":{"id":"jno8LbLfBHA4"}},{"cell_type":"code","source":["import os\n","import requests\n","\n","if not os.path.exists(file_path):\n","  url = (\n","      \"https://raw.githubusercontent.com/rasbt/\"\n","        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n","        \"the-verdict.txt\"\n","  )\n","  file_path = \"the-verdict.txt\"\n","  raw_text = \"\"\n","  response = requests.get(url, timeout=30)\n","  response.raise_for_status()\n","  with open(file_path, \"wb\") as f:\n","    f.write(response.content)\n"],"metadata":{"id":"1ufiwKMvBDc2","executionInfo":{"status":"ok","timestamp":1764338236949,"user_tz":-540,"elapsed":1,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["\n","with open(\"/content/drive/MyDrive/study/LLM/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n","    raw_text = f.read()\n","\n","print(\"총 문자 개수:\", len(raw_text))\n","print(raw_text[:99])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gh4N5VQ5DYAH","executionInfo":{"status":"ok","timestamp":1764338236960,"user_tz":-540,"elapsed":9,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"3f52560e-6531-4eff-cd47-ceef3e6cfe26"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["총 문자 개수: 20479\n","I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"]}]},{"cell_type":"markdown","source":["* LLM을 위해 텍스트를 토큰화, 임베딩\n","* 간단한 토크나이저 만들고 텍스트에 적용\n","\n","* 공백과 쉼표, 마침표, 구둣점등으로 문자처리 그리고 빈문자열 삭제\n"],"metadata":{"id":"7RJmj5t9C_ez"}},{"cell_type":"code","source":["import re\n","\n","text = \"Hello, world. Is this-- a test?\"\n","\n","result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","result = [item.strip() for item in result if item.strip()]\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fcQNeaED5Z9","executionInfo":{"status":"ok","timestamp":1764338236998,"user_tz":-540,"elapsed":36,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"86a1f61e-5dd7-48a0-e847-e46cf730b8c8"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"]}]},{"cell_type":"markdown","source":["토큰화 원시 텍스트에 적용"],"metadata":{"id":"MjL8VURNMHMU"}},{"cell_type":"code","source":["preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n","preprocessed = [item.strip() for item in preprocessed if item.strip()]\n","print(preprocessed[:30])\n","\n","print(\"총 개수 :\",len(preprocessed))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fOwvAWpRMJeO","executionInfo":{"status":"ok","timestamp":1764338237008,"user_tz":-540,"elapsed":9,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"a766cf5c-f16f-4f08-eaf3-cf3ae57c96f9"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n","총 개수 : 4690\n"]}]},{"cell_type":"markdown","source":["*이탤릭체 텍스트*# 새 섹션"],"metadata":{"id":"sVIfURVDPEog"}},{"cell_type":"markdown","source":["## **토큰을 토큰 ID로 변환하기**\n","\n"],"metadata":{"id":"kw0h0U6gPH-0"}},{"cell_type":"markdown","source":["* 토큰을 고유한 토큰으로 구성된 어휘 사전을 만들기\n"],"metadata":{"id":"Rbk7uCyKMuSJ"}},{"cell_type":"code","source":["all_words = sorted(set(preprocessed))\n","vocab_size = len(all_words)\n","\n","print(vocab_size)\n","vocab = {token:integer for integer,token in enumerate(all_words)}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HL_Qf1vBMuFr","executionInfo":{"status":"ok","timestamp":1764338237016,"user_tz":-540,"elapsed":7,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"30fb3e29-3b5c-4ce8-de9e-83b864fd3821"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["1130\n"]}]},{"cell_type":"markdown","source":["* 어휘사전을 만들었으니 50개 항목확인"],"metadata":{"id":"9OYGjyspM5RH"}},{"cell_type":"code","source":["for i, item in enumerate(vocab.items()):\n","    print(item)\n","    if i >= 50:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JdJrO48TMsX5","executionInfo":{"status":"ok","timestamp":1764338237025,"user_tz":-540,"elapsed":8,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"b300c5bf-a36f-4608-8145-439f37fecc52"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["('!', 0)\n","('\"', 1)\n","(\"'\", 2)\n","('(', 3)\n","(')', 4)\n","(',', 5)\n","('--', 6)\n","('.', 7)\n","(':', 8)\n","(';', 9)\n","('?', 10)\n","('A', 11)\n","('Ah', 12)\n","('Among', 13)\n","('And', 14)\n","('Are', 15)\n","('Arrt', 16)\n","('As', 17)\n","('At', 18)\n","('Be', 19)\n","('Begin', 20)\n","('Burlington', 21)\n","('But', 22)\n","('By', 23)\n","('Carlo', 24)\n","('Chicago', 25)\n","('Claude', 26)\n","('Come', 27)\n","('Croft', 28)\n","('Destroyed', 29)\n","('Devonshire', 30)\n","('Don', 31)\n","('Dubarry', 32)\n","('Emperors', 33)\n","('Florence', 34)\n","('For', 35)\n","('Gallery', 36)\n","('Gideon', 37)\n","('Gisburn', 38)\n","('Gisburns', 39)\n","('Grafton', 40)\n","('Greek', 41)\n","('Grindle', 42)\n","('Grindles', 43)\n","('HAD', 44)\n","('Had', 45)\n","('Hang', 46)\n","('Has', 47)\n","('He', 48)\n","('Her', 49)\n","('Hermia', 50)\n"]}]},{"cell_type":"markdown","source":["### 토큰화 하는 클래스 생성"],"metadata":{"id":"kvCBR9tJNVKd"}},{"cell_type":"code","source":["class SimpleTokenizerV1:\n","    def __init__(self, vocab):\n","        self.str_to_int = vocab\n","        self.int_to_str = {i:s for s,i in vocab.items()}\n","\n","    def encode(self, text):\n","        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # 'hello,. world'\n","\n","        preprocessed = [\n","            item.strip() for item in preprocessed if item.strip()\n","        ]\n","        ids = [self.str_to_int[s] for s in preprocessed]\n","        return ids\n","\n","    def decode(self, ids):\n","        text = \" \".join([self.int_to_str[i] for i in ids])\n","        # 구둣점 문자 앞의 공백을 삭제합니다.\n","        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","        return text\n","\n","'''\n","encode 함수는 텍스트를 토큰 ID로 바꿉니다.\n","decode 함수는 토큰 ID를 텍스트로 바꿉니다.\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"zilMBwBeNYF7","executionInfo":{"status":"ok","timestamp":1764338237032,"user_tz":-540,"elapsed":6,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"cb4de79d-add7-4d6c-c739-687418929d4a"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nencode 함수는 텍스트를 토큰 ID로 바꿉니다.\\ndecode 함수는 토큰 ID를 텍스트로 바꿉니다.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["### 토크나이저를 사용해 텍스트를 정수로 인코딩(토큰화)"],"metadata":{"id":"t6FQbWdyN-xO"}},{"cell_type":"markdown","source":["* 굵은 텍스트 이정수를 나중에 임베딩하여 LLM의 입력에 사용가능"],"metadata":{"id":"xn1jnzQSPlvN"}},{"cell_type":"code","source":["tokenizer = SimpleTokenizerV1(vocab)\n","\n","text = \"\"\"\"It's the last he painted, you know,\"\n","           Mrs. Gisburn said with pardonable pride.\"\"\"\n","ids = tokenizer.encode(text)\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"av-FVaDGOJA1","executionInfo":{"status":"ok","timestamp":1764338237057,"user_tz":-540,"elapsed":24,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"82590762-4abd-43cd-d86a-b1484265b376"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"]}]},{"cell_type":"markdown","source":["* 정수를 텍스트로 다시 디코딩하는 경우"],"metadata":{"id":"aUuiAKUbOL3J"}},{"cell_type":"code","source":["print(tokenizer.decode(ids))\n","print(tokenizer.decode(tokenizer.encode(text)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4yTD7lwmONan","executionInfo":{"status":"ok","timestamp":1764338237108,"user_tz":-540,"elapsed":50,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"e238a9a6-6c62-4004-9e13-1fd54259d61d"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n","\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"]}]},{"cell_type":"markdown","source":["## **특수 문맥 토큰 추가하기**"],"metadata":{"id":"h0VMrS_8P0Gu"}},{"cell_type":"markdown","source":["* 알지 못하는 단어와 텍스트의 끝을 알리는 \"특수\" 토큰을 추가하면 좋다"],"metadata":{"id":"q6WFUlteP8a1"}},{"cell_type":"code","source":["#어휘사전에 등록되지 않은 경우 오류 발생\n","tokenizer = SimpleTokenizerV1(vocab)\n","try:\n","    # 원본 코드\n","    text = \"Hello, do you like tea. Is this-- a test?\"\n","    tokenizer.encode(text)\n","except KeyError as e:\n","    # KeyError 발생 시 처리 로직\n","    print(f\"사전(Dictionary)에서 찾을 수 없는 문자열 발견: {e}\")\n","    # 추가 처리 옵션 (예: 특수문자 무시, 기본 ID 할당 등)\n","    # 예: 특수문자를 무시하고 빈 리스트 반환\n","    print(\"특수문자 무시 후 재처리 시도\")\n","\n","    # (선택사항) 대체 ID 사용 예시\n","    default_id = 0  # 사용자 정의 기본 ID\n","    encoded_text = []\n","    for char in text:\n","        encoded_text.append(tokenizer.str_to_int.get(char, default_id))\n","    print(f\"대체된 인코딩 결과: {encoded_text}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWVagcRnP67C","executionInfo":{"status":"ok","timestamp":1764338237123,"user_tz":-540,"elapsed":13,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"1a7f8fea-04d4-489b-91e0-eb23c469bdad"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["사전(Dictionary)에서 찾을 수 없는 문자열 발견: 'Hello'\n","특수문자 무시 후 재처리 시도\n","대체된 인코딩 결과: [0, 0, 0, 0, 0, 5, 0, 310, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 970, 0, 115, 7, 0, 53, 850, 0, 970, 0, 0, 850, 0, 0, 0, 115, 0, 970, 0, 850, 970, 10]\n"]}]},{"cell_type":"markdown","source":["* 이를 처리하기 위해 알지 못하는 단어를 표현하는 \"<|unk|>\" 같은 특수 토큰을 어휘 사전에 추가\n","* 어휘사전을 준비했으므로 GPT-2 훈련에서 텍스트의 끝을 나타내기 위해 사용된 \"<|endoftext|>\" 토큰을 추가"],"metadata":{"id":"7LCD46G-Rh7O"}},{"cell_type":"code","source":["all_tokens = sorted(list(set(preprocessed)))\n","all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n","\n","vocab = {token:integer for integer,token in enumerate(all_tokens)}"],"metadata":{"id":"yM5aYhEhRfO_","executionInfo":{"status":"ok","timestamp":1764338237127,"user_tz":-540,"elapsed":2,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}}},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":["새로운 <unk> 토큰을 사용할 때와 방법을 알려 주기 위해 토크나이저를 수정"],"metadata":{"id":"Gof5HKVAR2YY"}},{"cell_type":"code","source":["class SimpleTokenizerV2:\n","    def __init__(self, vocab):\n","        self.str_to_int = vocab\n","        self.int_to_str = { i:s for s,i in vocab.items()}\n","\n","    def encode(self, text):\n","        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n","        preprocessed = [\n","            item if item in self.str_to_int\n","            else \"<|unk|>\" for item in preprocessed\n","        ]\n","\n","        ids = [self.str_to_int[s] for s in preprocessed]\n","        return ids\n","\n","    def decode(self, ids):\n","        text = \" \".join([self.int_to_str[i] for i in ids])\n","        # 구둣점 문자 앞의 공백을 삭제합니다.\n","        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n","        return text"],"metadata":{"id":"_ePAfnghRz9-","executionInfo":{"status":"ok","timestamp":1764338237131,"user_tz":-540,"elapsed":2,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["tokenizer = SimpleTokenizerV2(vocab)\n","\n","text1 = \"Hello, do you like tea?\"\n","text2 = \"In the sunlit terraces of the palace.\"\n","\n","text = \" <|endoftext|> \".join((text1, text2))\n","\n","print(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"erx4vgWHR9Pn","executionInfo":{"status":"ok","timestamp":1764338237158,"user_tz":-540,"elapsed":25,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"2a29478e-8a2c-486f-db95-ff1bd0c29a23"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"]}]},{"cell_type":"code","source":["print(\"토큰화된 :\", tokenizer.encode(text))\n","print(\"디코드된 :\", tokenizer.decode(tokenizer.encode(text)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CZQXnLptSBL3","executionInfo":{"status":"ok","timestamp":1764338237159,"user_tz":-540,"elapsed":12,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"f5faa620-be1c-48b7-dc50-851c7ce4fe7a"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["토큰화된 : [1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n","디코드된 : <|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"]}]},{"cell_type":"markdown","source":["## 바이트 페어 인코딩"],"metadata":{"id":"ZFOzbptPSMeM"}},{"cell_type":"markdown","source":["* GPT-2는 바이트 페어 인코딩(BPE) 토크나이저 사용\n","* 어휘사전에 없는 단어를 더 작은 부분단어나 개별 문자로 분할하여 처리할 수 있다.\n","* BPE 토크나이저는 알지 못하는 단어를 부분단어나 개별 문자로 분할한다."],"metadata":{"id":"QWQKBZlTSdqD"}},{"cell_type":"code","source":["import tiktoken\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")"],"metadata":{"id":"EcQz_bWUSQU0","executionInfo":{"status":"ok","timestamp":1764338237160,"user_tz":-540,"elapsed":4,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["text = (\n","    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n","     \"of someunknownPlace.\"\n",")\n","\n","integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n","\n","print(integers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0z8qLxmSuO3","executionInfo":{"status":"ok","timestamp":1764338237167,"user_tz":-540,"elapsed":6,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"6a71fd1f-4f41-449e-c83e-ec29d088197e"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"]}]},{"cell_type":"code","source":["tokenizer.special_tokens_set"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_SdybiRiSwMO","executionInfo":{"status":"ok","timestamp":1764338237192,"user_tz":-540,"elapsed":22,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"acef53c0-a64c-4d17-88ab-7c3bbfc83670"},"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'<|endoftext|>'}"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["print(tokenizer.encode(text, allowed_special='all'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RpVRiDdpSzIn","executionInfo":{"status":"ok","timestamp":1764338237200,"user_tz":-540,"elapsed":7,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"b687127a-37d9-44cd-bce4-507f4ed11e2c"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"]}]},{"cell_type":"code","source":["strings = tokenizer.decode(integers)\n","print(strings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RjzWfx5US13j","executionInfo":{"status":"ok","timestamp":1764338237223,"user_tz":-540,"elapsed":20,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"76e2e779-de79-4131-b01e-cf3099806370"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"]}]},{"cell_type":"markdown","source":["## 슬라이딩 윈도로 데이터 샘플링하기"],"metadata":{"id":"Wi-6ACOzTKMX"}},{"cell_type":"code","source":["with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","    raw_text = f.read()\n","\n","enc_text = tokenizer.encode(raw_text)\n","print(len(enc_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nn-MT26GTQYK","executionInfo":{"status":"ok","timestamp":1764338237249,"user_tz":-540,"elapsed":24,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"ac2df414-23ca-4d80-885c-454649b74ba6"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["5145\n"]}]},{"cell_type":"markdown","source":["* 텍스트 청크에 대해 입력과 타깃이 있는지 확인\n","* 모델이 다음 단어를 예측하고 타깃은 오른쪽으로 한 토큰 이동한 입력"],"metadata":{"id":"9XKnM1-0TpQp"}},{"cell_type":"code","source":["enc_sample = enc_text[50:]\n","context_size = 4\n","\n","x = enc_sample[:context_size]\n","y = enc_sample[1:context_size+1]\n","\n","print(f\"x: {x}\")\n","print(f\"y:      {y}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G6I1niS5To4m","executionInfo":{"status":"ok","timestamp":1764338237263,"user_tz":-540,"elapsed":13,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"fa99470f-81c6-43be-968f-01e1056bea2a"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["x: [290, 4920, 2241, 287]\n","y:      [4920, 2241, 287, 257]\n"]}]},{"cell_type":"markdown","source":["* 한 번에 하나씩 예측은 다음과 같이 수행"],"metadata":{"id":"-XlUC3K1T5AY"}},{"cell_type":"code","source":["for i in range(1, context_size+1):\n","    context = enc_sample[:i]\n","    desired = enc_sample[i]\n","\n","    print(context, \"---->\", desired)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CCkzvLcKT3_G","executionInfo":{"status":"ok","timestamp":1764338237270,"user_tz":-540,"elapsed":6,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"5f2ee6ea-1304-4ffc-c2e7-e5ec6588ee79"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["[290] ----> 4920\n","[290, 4920] ----> 2241\n","[290, 4920, 2241] ----> 287\n","[290, 4920, 2241, 287] ----> 257\n"]}]},{"cell_type":"code","source":["for i in range(1, context_size+1):\n","    context = enc_sample[:i]\n","    desired = enc_sample[i]\n","\n","    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"baNy8enAT9u-","executionInfo":{"status":"ok","timestamp":1764338237278,"user_tz":-540,"elapsed":6,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"b95af894-25ba-4d52-af1b-7dc17ace654b"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":[" and ---->  established\n"," and established ---->  himself\n"," and established himself ---->  in\n"," and established himself in ---->  a\n"]}]},{"cell_type":"markdown","source":["* 데이터셋을 순회하면서 입력과 타깃을 반환하는  간단한 데이터 로더 구현\n","* 데이터 셋과 입력 텍스트 데이터셋에서 청크를 추출하는 데이터 로더를 만든다."],"metadata":{"id":"Ed0kSiX6T44a"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","class GPTDatasetV1(Dataset):\n","    def __init__(self, txt, tokenizer, max_length, stride):\n","        self.input_ids = []\n","        self.target_ids = []\n","\n","        # 전체 텍스트를 토큰화합니다.\n","        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n","        assert len(token_ids) > max_length, \"토큰화된 입력의 개수는 적어도 max_length+1과 같아야 합니다.\"\n","\n","        # 슬라이딩 윈도를 사용해 책을 max_length 길이의 중첩된 시퀀스로 나눕니다.\n","        for i in range(0, len(token_ids) - max_length, stride):\n","            input_chunk = token_ids[i:i + max_length]\n","            target_chunk = token_ids[i + 1: i + max_length + 1]\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.target_ids[idx]"],"metadata":{"id":"iyxRhySoUXg9","executionInfo":{"status":"ok","timestamp":1764338237281,"user_tz":-540,"elapsed":1,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["def create_dataloader_v1(txt, batch_size=4, max_length=256,\n","                         stride=128, shuffle=True, drop_last=True,\n","                         num_workers=0):\n","\n","    # 토크나이저를 초기화합니다.\n","    tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","    # 데이터셋을 만듭니다.\n","    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n","\n","    # 데이터 로더를 만듭니다.\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers\n","    )\n","\n","    return dataloader"],"metadata":{"id":"sv44c58vUbko","executionInfo":{"status":"ok","timestamp":1764338237292,"user_tz":-540,"elapsed":9,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}}},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":["* 문맥 크기를 4와 배치 크기 1로 데이터 로더를 테스트한 결과"],"metadata":{"id":"OQorxwAFUeCy"}},{"cell_type":"code","source":["dataloader = create_dataloader_v1(\n","    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",")\n","\n","data_iter = iter(dataloader)\n","first_batch = next(data_iter)\n","print(first_batch)\n","\n","second_batch = next(data_iter)\n","print(second_batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b4iAeRKoUfPI","executionInfo":{"status":"ok","timestamp":1764338237359,"user_tz":-540,"elapsed":65,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"e96ddfcf-a36a-494e-da5e-7a2e33b3d317"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n","[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"]}]},{"cell_type":"markdown","source":["* 배치 출력도 만들수 있다.\n","* 배치 간에 중첩이 있으면 과대적합이 증가될 수 있으므로 스트라이드를 증가"],"metadata":{"id":"D17oOSXkUuHV"}},{"cell_type":"code","source":["dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n","\n","data_iter = iter(dataloader)\n","inputs, targets = next(data_iter)\n","print(\"입력:\\n\", inputs)\n","print(\"\\n타깃:\\n\", targets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Az5vjLTRUpLP","executionInfo":{"status":"ok","timestamp":1764338237394,"user_tz":-540,"elapsed":33,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"e3b1afd7-450f-4488-f7c5-0a9dc4a4bc8b"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["입력:\n"," tensor([[   40,   367,  2885,  1464],\n","        [ 1807,  3619,   402,   271],\n","        [10899,  2138,   257,  7026],\n","        [15632,   438,  2016,   257],\n","        [  922,  5891,  1576,   438],\n","        [  568,   340,   373,   645],\n","        [ 1049,  5975,   284,   502],\n","        [  284,  3285,   326,    11]])\n","\n","타깃:\n"," tensor([[  367,  2885,  1464,  1807],\n","        [ 3619,   402,   271, 10899],\n","        [ 2138,   257,  7026, 15632],\n","        [  438,  2016,   257,   922],\n","        [ 5891,  1576,   438,   568],\n","        [  340,   373,   645,  1049],\n","        [ 5975,   284,   502,   284],\n","        [ 3285,   326,    11,   287]])\n"]}]},{"cell_type":"markdown","source":["## 토큰 임베딩 만들기"],"metadata":{"id":"uqH2ywm0UzUk"}},{"cell_type":"markdown","source":["* 임베딩 층을 사용해 토큰을 연속적일 벡터 표현으로 임베딩한다.\n","* 이런 임베딩 층은 LLM의 일부이며, 모델 훈련 과정에서 업데이트 된다."],"metadata":{"id":"7eLV9XJSU-cl"}},{"cell_type":"code","source":["input_ids = torch.tensor([2, 3, 5, 1])"],"metadata":{"id":"vtxGX-SGU1Rf","executionInfo":{"status":"ok","timestamp":1764338237423,"user_tz":-540,"elapsed":3,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}}},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":["* 테스트 : 어휘사전에는 6개 단어만 있고, 임베딩 크기는 3인 경우\n","\n","결과적으로 6x3 가중치 행렬이 만들어진다."],"metadata":{"id":"TJosUYu3VDhl"}},{"cell_type":"code","source":["vocab_size = 6\n","output_dim = 3\n","\n","torch.manual_seed(123)\n","embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n","print(embedding_layer.weight)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kmgPUsYmVCyK","executionInfo":{"status":"ok","timestamp":1764338237439,"user_tz":-540,"elapsed":15,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"1b129dc7-4e27-4561-e537-a01329dcd93c"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[ 0.3374, -0.1778, -0.1690],\n","        [ 0.9178,  1.5810,  1.3010],\n","        [ 1.2753, -0.2010, -0.1606],\n","        [-0.4015,  0.9666, -1.1481],\n","        [-1.1589,  0.3255, -0.6315],\n","        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["## 단어 위치 인코딩하기"],"metadata":{"id":"7iyzcg89nHmv"}},{"cell_type":"markdown","source":["* 임베딩 층은 토큰 ID를 입력 시퀀스에서 어떤 위치에 있던지 상관없이 동일한 벡터 표현으로 바꾼다.\n","* 위치 임베딩을 토큰 임베딩에 더해서 대규모 언어 모델을 위한 입력 임베딩 만들기\n","* 바이트 페어 인코더의 어휘 사전 크기 구하기\n","* 입력 토큰 256차원의 벡터표현으로 인코딩"],"metadata":{"id":"XyaHu6_HnN_u"}},{"cell_type":"code","source":["vocab_size = 50257\n","output_dim = 256\n","\n","token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"],"metadata":{"id":"S-csmFapnHQg","executionInfo":{"status":"ok","timestamp":1764338237755,"user_tz":-540,"elapsed":315,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}}},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":["* 데이터 로더에서 데이터를 샘플링한 다음 배치에 있는 각 샘플의 토큰을 256차원 벡터로 임베딩할 수 있다.\n","* 배치 크기가 8이고 샘플마다 4개의 토큰이 있다면 8 x 4 256 크기의 텐서가 만들어진다."],"metadata":{"id":"3gLl1YmsnkpS"}},{"cell_type":"code","source":["max_length = 4\n","dataloader = create_dataloader_v1(\n","    raw_text, batch_size=8, max_length=max_length,\n","    stride=max_length, shuffle=False\n",")\n","data_iter = iter(dataloader)\n","inputs, targets = next(data_iter)"],"metadata":{"id":"6NU7WzGvofHE","executionInfo":{"status":"ok","timestamp":1764338237807,"user_tz":-540,"elapsed":50,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["print(\"토큰 ID:\\n\", inputs)\n","print(\"\\n입력 크기:\\n\", inputs.shape)"],"metadata":{"id":"KmBJ9pHGVLgU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764338237844,"user_tz":-540,"elapsed":13,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"0284b037-7e98-442d-87fc-dd68754f31e7"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["토큰 ID:\n"," tensor([[   40,   367,  2885,  1464],\n","        [ 1807,  3619,   402,   271],\n","        [10899,  2138,   257,  7026],\n","        [15632,   438,  2016,   257],\n","        [  922,  5891,  1576,   438],\n","        [  568,   340,   373,   645],\n","        [ 1049,  5975,   284,   502],\n","        [  284,  3285,   326,    11]])\n","\n","입력 크기:\n"," torch.Size([8, 4])\n"]}]},{"cell_type":"code","source":["token_embeddings = token_embedding_layer(inputs)\n","print(token_embeddings.shape)\n","\n","# 임베딩 벡터의 값을 확인합니다.\n","print(token_embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wiSlSyYloUyb","executionInfo":{"status":"ok","timestamp":1764338238000,"user_tz":-540,"elapsed":153,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"8201b08e-07fb-4bac-8d76-49be3c811623"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 4, 256])\n","tensor([[[ 0.4913,  1.1239,  1.4588,  ..., -0.3995, -1.8735, -0.1445],\n","         [ 0.4481,  0.2536, -0.2655,  ...,  0.4997, -1.1991, -1.1844],\n","         [-0.2507, -0.0546,  0.6687,  ...,  0.9618,  2.3737, -0.0528],\n","         [ 0.9457,  0.8657,  1.6191,  ..., -0.4544, -0.7460,  0.3483]],\n","\n","        [[ 1.5460,  1.7368, -0.7848,  ..., -0.1004,  0.8584, -0.3421],\n","         [-1.8622, -0.1914, -0.3812,  ...,  1.1220, -0.3496,  0.6091],\n","         [ 1.9847, -0.6483, -0.1415,  ..., -0.3841, -0.9355,  1.4478],\n","         [ 0.9647,  1.2974, -1.6207,  ...,  1.1463,  1.5797,  0.3969]],\n","\n","        [[-0.7713,  0.6572,  0.1663,  ..., -0.8044,  0.0542,  0.7426],\n","         [ 0.8046,  0.5047,  1.2922,  ...,  1.4648,  0.4097,  0.3205],\n","         [ 0.0795, -1.7636,  0.5750,  ...,  2.1823,  1.8231, -0.3635],\n","         [ 0.4267, -0.0647,  0.5686,  ..., -0.5209,  1.3065,  0.8473]],\n","\n","        ...,\n","\n","        [[-1.6156,  0.9610, -2.6437,  ..., -0.9645,  1.0888,  1.6383],\n","         [-0.3985, -0.9235, -1.3163,  ..., -1.1582, -1.1314,  0.9747],\n","         [ 0.6089,  0.5329,  0.1980,  ..., -0.6333, -1.1023,  1.6292],\n","         [ 0.3677, -0.1701, -1.3787,  ...,  0.7048,  0.5028, -0.0573]],\n","\n","        [[-0.1279,  0.6154,  1.7173,  ...,  0.3789, -0.4752,  1.5258],\n","         [ 0.4861, -1.7105,  0.4416,  ...,  0.1475, -1.8394,  1.8755],\n","         [-0.9573,  0.7007,  1.3579,  ...,  1.9378, -1.9052, -1.1816],\n","         [ 0.2002, -0.7605, -1.5170,  ..., -0.0305, -0.3656, -0.1398]],\n","\n","        [[-0.9573,  0.7007,  1.3579,  ...,  1.9378, -1.9052, -1.1816],\n","         [-0.0632, -0.6548, -1.0296,  ..., -0.9538, -0.5026, -0.1128],\n","         [ 0.6032,  0.8983,  2.0722,  ...,  1.5242,  0.2030, -0.3002],\n","         [ 1.1274, -0.1082, -0.2195,  ...,  0.5059, -1.8138, -0.0700]]],\n","       grad_fn=<EmbeddingBackward0>)\n"]}]},{"cell_type":"markdown","source":["* GPT-2는 절대 위치 임베딩을 사용하므로 또 다른 임베딩 층을 만들면 됩니다:\n"],"metadata":{"id":"19n61w0QoXIE"}},{"cell_type":"code","source":["context_length = max_length\n","pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n","\n","# 임베딩 층의 가중치를 확인합니다.\n","print(pos_embedding_layer.weight)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ye6NHyjsoZLE","executionInfo":{"status":"ok","timestamp":1764338238013,"user_tz":-540,"elapsed":10,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"d4a644e0-bdab-4a24-fc6f-da7ee9413c2b"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n","        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n","        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n","        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n","       requires_grad=True)\n"]}]},{"cell_type":"code","source":["pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n","print(pos_embeddings.shape)\n","\n","# 위치 임베딩 값을 확인합니다.\n","print(pos_embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"romy1RKGoy3n","executionInfo":{"status":"ok","timestamp":1764338238033,"user_tz":-540,"elapsed":6,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"93069651-0165-492c-8260-42d3f66afd52"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 256])\n","tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n","        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n","        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n","        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n","       grad_fn=<EmbeddingBackward0>)\n"]}]},{"cell_type":"markdown","source":["LLM에 사용될 입력 임베딩을 만들기 위해 토큰 임베딩과 위치 임베딩을 더한다."],"metadata":{"id":"_Fhgt6GIo5w3"}},{"cell_type":"code","source":["input_embeddings = token_embeddings + pos_embeddings\n","print(input_embeddings.shape)\n","\n","# 입력 임베딩 값을 확인합니다.\n","print(input_embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2fjV6afBo1bV","executionInfo":{"status":"ok","timestamp":1764338238043,"user_tz":-540,"elapsed":8,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"57e309c2-db15-4b50-d7c1-07286a02053a"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 4, 256])\n","tensor([[[ 2.2288,  0.5619,  0.8286,  ..., -0.6272, -0.2987,  0.8900],\n","         [ 2.0903, -0.4664, -0.0593,  ...,  0.9115, -1.0493, -1.6473],\n","         [-0.7158, -0.8304,  1.2494,  ...,  2.3952,  1.8773,  0.8051],\n","         [ 0.2703,  0.4029,  3.0514,  ...,  0.3595, -1.4548,  0.8310]],\n","\n","        [[ 3.2835,  1.1749, -1.4150,  ..., -0.3281,  2.4332,  0.6924],\n","         [-0.2199, -0.9114, -0.1750,  ...,  1.5337, -0.1998,  0.1462],\n","         [ 1.5197, -1.4240,  0.4391,  ...,  1.0494, -1.4318,  2.3057],\n","         [ 0.2893,  0.8346, -0.1884,  ...,  1.9602,  0.8709,  0.8796]],\n","\n","        [[ 0.9662,  0.0952, -0.4640,  ..., -1.0320,  1.6290,  1.7771],\n","         [ 2.4468, -0.2154,  1.4984,  ...,  1.8766,  0.5595, -0.1423],\n","         [-0.3856, -2.5393,  1.1556,  ...,  3.6157,  1.3267,  0.4944],\n","         [-0.2487, -0.5275,  2.0009,  ...,  0.2930,  0.5977,  1.3300]],\n","\n","        ...,\n","\n","        [[ 0.1219,  0.3991, -3.2740,  ..., -1.1921,  2.6637,  2.6728],\n","         [ 1.2438, -1.6436, -1.1101,  ..., -0.7464, -0.9816,  0.5118],\n","         [ 0.1439, -0.2428,  0.7786,  ...,  0.8001, -1.5986,  2.4871],\n","         [-0.3077, -0.6329,  0.0536,  ...,  1.5188, -0.2060,  0.4254]],\n","\n","        [[ 1.6095,  0.0535,  1.0871,  ...,  0.1512,  1.0996,  2.5603],\n","         [ 2.1284, -2.4306,  0.6478,  ...,  0.5593, -1.6896,  1.4126],\n","         [-1.4224, -0.0750,  1.9386,  ...,  3.3712, -2.4016, -0.3237],\n","         [-0.4752, -1.2234, -0.0847,  ...,  0.7834, -1.0744,  0.3429]],\n","\n","        [[ 0.7802,  0.1387,  0.7277,  ...,  1.7101, -0.3304, -0.1471],\n","         [ 1.5791, -1.3749, -0.8234,  ..., -0.5420, -0.3528, -0.5756],\n","         [ 0.1382,  0.1226,  2.6528,  ...,  2.9576, -0.2933,  0.5577],\n","         [ 0.4520, -0.5711,  1.2128,  ...,  1.3198, -2.5226,  0.4127]]],\n","       grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"markdown","source":["* 입력 전처리 워크플로의 초기 단계에서 입력테스트를 별개의 토큰으로 분할한다.\n","* 이 분할 단계 다음에 사전에 정의된 어휘 사전을 기반으로 토큰을 토큰 ID로 변환한다."],"metadata":{"id":"JZ3z2aHhpCzb"}}]}