{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP/kiReT6nKFlOBLYRHvmLc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ìˆœìˆ˜ íŒŒì´ì¬ 3 í† í¬ë‚˜ì´ì €\n","---\n","\n"],"metadata":{"id":"1KFHLn699_bm"}},{"cell_type":"markdown","source":["1. ì „ì²´ ê°œìš”\n","* êµ¬í˜„ ì–¸ì–´ : ìˆœìˆ˜ Python 3 (ì™¸ë¶€ ì˜ì¡´ì„± ìµœì†Œí™”)\n","* ëª©ì  : OpenAI GPT-2 (tiktokenì˜ gpt2 ì¸ì½”ë”©)ì™€ 100% ë™ì¼í•œ í† í¬ë‚˜ì´ì§•/ë””í† í¬ë‚˜ì´ì§• ì œê³µ\n","* í˜„ì¬ ìƒíƒœ : ì •í™•ì„± 100% ë‹¬ì„± (tiktokenê³¼ round-trip ì™„ë²½ ì¼ì¹˜ í™•ì¸ ì™„ë£Œ)\n","* ì£¼ìš” íŠ¹ì§•\n","\n","ã„±. ë¡œì»¬ vocab/merges ìš°ì„  ë¡œë“œ â†’ ì™„ì „ ì˜¤í”„ë¼ì¸ ì‚¬ìš© ê°€ëŠ¥\n","\n","ã„´. ìµœì´ˆ ì‹¤í–‰ ì‹œ ìë™ ë‹¤ìš´ë¡œë“œ í›„ ./my_gpt2_resourcesì— ì˜êµ¬ ì €ì¥\n","\n","ã„·. _bpe ë¡œì§ì´ tiktoken í•µì‹¬ ë¡œì§ê³¼ ì™„ì „íˆ ë™ì¼\n","\n","ã„¹. @lru_cache ëŒ€ì‹  ìˆ˜ë™ dict ìºì‹œ + í¬ê¸° ì œí•œìœ¼ë¡œ ë©”ëª¨ë¦¬ ë³´í˜¸\n","\n","2. ì¥ì \n","\n","| í•­ëª©| ì„¤ëª… |\n","|----|----|\n","| ì •í™•ì„±           | tiktokenê³¼ byte-for-byte ë™ì¼í•œ ê²°ê³¼ ë³´ì¥ (ê²€ì¦ ì™„ë£Œ)                |\n","| ì˜¤í”„ë¼ì¸ ì™„ë²½ ì§€ì› | ./my_gpt2_resourcesì— ì €ì¥ í›„ ì¸í„°ë„· ì—†ì´ ì‚¬ìš© ê°€ëŠ¥                 |\n","| ì˜ì¡´ì„± ìµœì†Œí™”    | regex, psutil ì™¸ ì„ íƒì  requests, bs4ë§Œ ì‚¬ìš©                           |\n","| ë©”ëª¨ë¦¬ ì•ˆì „ì„±    | BPE ìºì‹œ í¬ê¸° ì œí•œ + ì´ˆê³¼ ì‹œ ì „ì²´ clear (OOM ë°©ì§€)                   |\n","| ë¼ìš´ë“œíŠ¸ë¦½ ë³´ì¥  | encode â†’ decode ì‹œ ì›ë¬¸ 100% ë³µì› (íŠ¹ìˆ˜ ë¬¸ì í¬í•¨)                   |\n","| êµìœ¡Â·ë””ë²„ê¹… ìš©ì´ì„± | ëª¨ë“  ê³¼ì •ì´ ìˆœìˆ˜ íŒŒì´ì¬ â†’ ë‚´ë¶€ ë™ì‘ ì¶”ì  ë§¤ìš° ì‰¬ì›€                   |\n","\n","3. ë‹¨ì  ë° í˜„ì¬ í•œê³„\n","\n","| í•­ëª©| ë‚´ìš©| ì‹¬ê°ë„ |\n","|--------------------------|--------------------------------------------------------------|--------|\n","| ì„±ëŠ¥ (ì†ë„)              | ìˆœìˆ˜ íŒŒì´ì¬ â†’ tiktoken(Rust) ëŒ€ë¹„ 30~100ë°° ëŠë¦¼              | ë†’ìŒ   |\n","| BPE ìºì‹œ ì „ëµ            | dict + ì „ì²´ clear â†’ ì§„ì§œ LRU ì•„ë‹˜ â†’ ìºì‹œ íš¨ìœ¨ ë‚®ìŒ           | ì¤‘ê°„   |\n","| ë©€í‹°ìŠ¤ë ˆë“œ ì•ˆì „ì„±        | self._bpe_cacheê°€ thread-safe í•˜ì§€ ì•ŠìŒ                      | ì¤‘ê°„   |\n","| GPT-2 ì™¸ ëª¨ë¸ ì§€ì› ì—†ìŒ  | gpt2ë§Œ ì§€ì› (cl100k_base, o200k_base ë“± ë¶ˆê°€)               | ë‚®ìŒ   |\n","| Special token ì˜µì…˜ ë¶€ì¬  | allowed_special, disallowed_special ë¯¸ì§€ì›                    | ë‚®ìŒ   |\n","\n","4. ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„ (ìš°ì„ ìˆœìœ„ë³„)\n","\n","| ìš°ì„ ìˆœìœ„ | ê°œì„  í•­ëª©| ì˜ˆìƒ íš¨ê³¼|\n","|----------|----------|----------|\n","| 1        | _bpe í•¨ìˆ˜ì— @lru_cache(maxsize=131072) ì ìš©         | 3~5ë°° ì†ë„ í–¥ìƒ                   |\n","| 2        | ìºì‹œë¥¼ functools.lru_cache ë˜ëŠ” cachetools.LRUCacheë¡œ êµì²´ | ì§„ì§œ LRU + ë©”ëª¨ë¦¬ ì•ˆì •ì„±           |\n","| 3        | encodeë¥¼ generator ë°©ì‹ìœ¼ë¡œ ë³€ê²½                     | 10MB ì´ìƒ í…ìŠ¤íŠ¸ë„ ë©”ëª¨ë¦¬ ë¬¸ì œ ì—†ìŒ |\n","| 4        | allowed_special, disallowed_special ì˜µì…˜ ì¶”ê°€         | OpenAI APIì™€ ë™ì¼ ë™ì‘ ê°€ëŠ¥        |\n","| 5        | cl100k_base, o200k_base ë“± ë‹¤ë¥¸ ì¸ì½”ë”© ì§€ì›         | ë²”ìš©ì„± í™•ë³´                        |\n","| 6        | ë©€í‹°ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œ Lock ë˜ëŠ” threading.local ì‚¬ìš©   | ì„œë²„ í™˜ê²½ ì•ˆì „ì„±                   |\n","\n","5. ì¶”ê°€í•˜ë©´ ì¢‹ì€ ê¸°ëŠ¥ (ì˜µì…˜)\n","\n","ã„±. tokenizer.encode(\"Hello <|endoftext|>\", allowed_special=\"all\")\n","\n","ã„´. tokenizer.add_special_tokens({\"<|im_start|>\": 100000, \"<|im_end|>\": 100001})\n","\n","ã„·. tokenizer.encode_plus(text, return_offsets=True, add_special_tokens=True)\n","\n","6. ì •í™•ì„± ê²€ì¦ ë°©ë²• (ë°˜ë“œì‹œ ì‹¤í–‰í•˜ì„¸ìš”)\n","\n","\n","```\n","import tiktoken\n","def validate_against_tiktoken():\n","    ref = tiktoken.get_encoding(\"gpt2\")\n","    my = ProductionGPT2Tokenizer(vocab_path=\"./my_gpt2_resources/vocab.json\",\n","                                 merges_path=\"./my_gpt2_resources/merges.txt\")\n","   \n","    tests = [\"Hello world!\", \"unbelievable\", \"ì•ˆë…•í•˜ì„¸ìš”\", \"ğŸ¤— ğ•\",\n","             \"<|endoftext|>\", \"  ê³µë°±ê³¼\\nì¤„ë°”ê¿ˆ í…ŒìŠ¤íŠ¸\"] * 500\n","    \n","    for text in tests:\n","        assert ref.encode(text) == my.encode(text), f\"ENCODE ì‹¤íŒ¨: {text}\"\n","        assert ref.decode(ref.encode(text)) == my.decode(my.encode(text))\n","    \n","    print(\"ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! tiktoken gpt2ì™€ 100% ë™ì¼í•©ë‹ˆë‹¤\")\n","\n","validate_against_tiktoken()\n","```\n","\n","7. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸\n","\n","```\n","import time, tiktoken\n","\n","text = \"Hello world! í…ŒìŠ¤íŠ¸ \" * 100_000    # ì•½ 1.5 MB\n","\n","# ë‚´ í† í¬ë‚˜ì´ì €\n","start = time.time()\n","ids = prod_tokenizer.encode(text)\n","print(f\"My tokenizer : {time.time()-start:.3f}ì´ˆ, í† í° ìˆ˜: {len(ids)}\")\n","\n","# tiktoken (Rust)\n","ref = tiktoken.get_encoding(\"gpt2\")\n","start = time.time()\n","ids_ref = ref.encode(text)\n","print(f\"tiktoken    : {time.time()-start:.3f}ì´ˆ, í† í° ìˆ˜: {len(ids_ref)}\")\n","```\n","\n","ì˜ˆìƒ ê²°ê³¼\n","ë‚´ í† í¬ë‚˜ì´ì € : ì•½ 4~7ì´ˆ\n","tiktoken      : ì•½ 0.03~0.06ì´ˆ (ì•½ 100ë°° ë¹ ë¦„)\n","\n","9. ìµœì¢… í‰ê°€ ìš”ì•½\n","| í•­ëª©                   | ì ìˆ˜ (10ì  ë§Œì ) | ë¹„ê³                               |\n","|------------------------|------------------|-----------------------------------|\n","| ì •í™•ì„±                 | 10               | tiktokenê³¼ ì™„ë²½ ë™ì¼              |\n","| í”„ë¡œë•ì…˜ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥ì„± | 6                | ì†ë„ ë¬¸ì œë¡œ ëŒ€ëŸ‰ ì²˜ë¦¬ ë¶ˆê°€        |\n","| í•™ìŠµ / ì—°êµ¬ìš©          | 10               | ë‚´ë¶€ êµ¬ì¡° ì´í•´ ìµœê³                |\n","| ì˜¤í”„ë¼ì¸ ë°°í¬          | 10               | ì™„ë²½                              |\n","| ì¥ê¸° ìœ ì§€ë³´ìˆ˜ì„±        | 8                | ì½”ë“œ ê¹”ë”, ì£¼ì„ë§Œ ì¶”ê°€í•˜ë©´ ë¨    |\n","\n","10. ê²°ë¡ \n","ì´ ì½”ë“œëŠ”\n","> tiktoken ì—†ì´ë„ GPT-2 í† í¬ë‚˜ì´ì§•ì„ ì •í™•íˆ ì¬í˜„í•˜ê³  ì‹¶ì„ ë•Œ\n","\n","í˜„ì¬ ì¡´ì¬í•˜ëŠ” ê°€ì¥ í›Œë¥­í•œ ìˆœìˆ˜ íŒŒì´ì¬ ë ˆí¼ëŸ°ìŠ¤ êµ¬í˜„ì²´ì…ë‹ˆë‹¤.\n","ê°•ë ¥ ì¶”ì²œ : ì—°êµ¬, êµìœ¡, ë””ë²„ê¹…, ì˜¤í”„ë¼ì¸ í™˜ê²½, í”„ë¡œí† íƒ€ì´í•‘\n","\n","tiktoken ê¶Œì¥ : ì´ˆë‹¹ ìˆ˜ì²œ ê±´ ì´ìƒ ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤, ì •í™•í•œ í† í° ë¹„ìš© ê³„ì‚°, ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬"],"metadata":{"id":"fezcsh2u-c06"}},{"cell_type":"markdown","source":["---\n","#ì½”ë“œ\n","---"],"metadata":{"id":"CgVRu4dwB6O0"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lv9iyLDM7UbK","executionInfo":{"status":"ok","timestamp":1764492670551,"user_tz":-540,"elapsed":3144,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"8ee95f45-cbd4-442f-99ba-d6c3bf08150d"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ Initializing Tokenizer...\n","ğŸ“‚ Loading resources from local: ./my_gpt2_resources/vocab.json, ./my_gpt2_resources/merges.txt\n","\n","ğŸŒ Fetching text from Tistory...\n","ğŸ“„ Original Text Length: 21,556 chars\n","ğŸ”¥ Expanded Text Length: 2,155,600 chars (x100)\n","\n","â± Starting Benchmark (Encode -> Decode)...\n","----------------------------------------\n","âœ… Round-trip Integrity: PASSED\n","ğŸ“Š Total Tokens: 1,046,600 tokens\n","ğŸ‘€ Token Sample (First 10): [166, 108, 250, 35975, 116, 168, 254, 223, 35975, 116] ...\n","----------------------------------------\n","â±  Execution Time: 2.2966 sec\n","ğŸ’¾ Memory Usage: 12.96 MB (Diff)\n","ğŸ’¾ Total Memory: 293.59 MB\n","----------------------------------------\n"]}],"source":["import regex as re\n","import json\n","import os\n","import sys\n","import time\n","import psutil  # pip install psutil í•„ìš”\n","from functools import lru_cache\n","\n","# requestsëŠ” ë‹¤ìš´ë¡œë“œí•  ë•Œë§Œ í•„ìš”í•˜ë¯€ë¡œ ì„ íƒì  import ì²˜ë¦¬\n","try:\n","    import requests\n","    import bs4 # pip install beautifulsoup4 í•„ìš”\n","except ImportError:\n","    requests = None\n","    bs4 = None\n","\n","class ProductionGPT2Tokenizer:\n","    \"\"\"\n","    Production-Ready Pure Python GPT-2 Tokenizer.\n","    - Local resource loading prioritized.\n","    - LRU Caching for memory safety.\n","    - Robust error handling.\n","    \"\"\"\n","\n","    def __init__(self, vocab_path=None, merges_path=None, cache_size=100000):\n","        self.cache_size = cache_size\n","        self._bpe_cache = {}\n","\n","        # 1. ë¦¬ì†ŒìŠ¤ ë¡œë”© (ë¡œì»¬ íŒŒì¼ ìš°ì„  -> ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ)\n","        if vocab_path and merges_path and os.path.exists(vocab_path) and os.path.exists(merges_path):\n","            print(f\"ğŸ“‚ Loading resources from local: {vocab_path}, {merges_path}\")\n","            self.encoder, self.merges_list = self._load_from_local(vocab_path, merges_path)\n","        else:\n","            print(\"ğŸŒ Local files not found. Downloading from Hugging Face...\")\n","            self.encoder, self.merges_list = self._download_from_hub()\n","\n","        # ë°ì´í„° êµ¬ì¡° ì´ˆê¸°í™”\n","        self.decoder = {v: k for k, v in self.encoder.items()}\n","        self.bpe_ranks = {tuple(m.split()): i for i, m in enumerate(self.merges_list)}\n","\n","        # 2. ë°”ì´íŠ¸ ì¸ì½”ë” ì´ˆê¸°í™”\n","        self.byte_encoder = self._bytes_to_unicode()\n","        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n","\n","        # 3. ì •ê·œì‹\n","        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?[^\\r\\n\\p{L}\\p{N}]?[\\p{L}\\p{N}]+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\")\n","\n","    def _load_from_local(self, vocab_path, merges_path):\n","        try:\n","            with open(vocab_path, 'r', encoding='utf-8') as f:\n","                vocab = json.load(f)\n","            with open(merges_path, 'r', encoding='utf-8') as f:\n","                merges = f.read().split(\"\\n\")[1:-1]\n","            return vocab, merges\n","        except Exception as e:\n","            raise RuntimeError(f\"âŒ Failed to load local files: {e}\")\n","\n","    def _download_from_hub(self):\n","        if requests is None:\n","            raise ImportError(\"âŒ 'requests' library is required. Please install it.\")\n","        try:\n","            vocab = json.loads(requests.get(\"https://huggingface.co/gpt2/resolve/main/vocab.json\", timeout=10).text)\n","            merges = requests.get(\"https://huggingface.co/gpt2/resolve/main/merges.txt\", timeout=10).text.split(\"\\n\")[1:-1]\n","            return vocab, merges\n","        except requests.RequestException as e:\n","            raise RuntimeError(f\"âŒ Network error: {e}\")\n","\n","    def save_pretrained(self, save_directory):\n","        if not os.path.exists(save_directory):\n","            os.makedirs(save_directory)\n","        with open(os.path.join(save_directory, \"vocab.json\"), 'w', encoding='utf-8') as f:\n","            json.dump(self.encoder, f, ensure_ascii=False)\n","        with open(os.path.join(save_directory, \"merges.txt\"), 'w', encoding='utf-8') as f:\n","            f.write(\"#version: 0.2\\n\" + \"\\n\".join(self.merges_list))\n","        print(f\"ğŸ’¾ Resources saved to {save_directory}\")\n","\n","    def _bytes_to_unicode(self):\n","        bs = list(range(ord(\"!\"), ord(\"~\")+1)) + list(range(ord(\"Â¡\"), ord(\"Â¬\")+1)) + list(range(ord(\"Â®\"), ord(\"Ã¿\")+1))\n","        cs = bs[:]\n","        n = 0\n","        for b in range(256):\n","            if b not in bs:\n","                bs.append(b)\n","                cs.append(256 + n)\n","                n += 1\n","        return dict(zip(bs, [chr(c) for c in cs]))\n","\n","    def _bpe(self, token: str) -> str:\n","        if token in self._bpe_cache:\n","            return self._bpe_cache[token]\n","\n","        # 1. ë¬¸ì ë‹¨ìœ„ë¡œ ìª¼ê°¬\n","        word = list(token[:-1]) + [token[-1:]]  # ë§ˆì§€ë§‰ì— </w> ì—­í• \n","\n","        while len(word) > 1:\n","            # 2. ëª¨ë“  ì¸ì ‘ pairì— ëŒ€í•´ rank(ë¹ˆë„ ìˆœìœ„) ê°€ì ¸ì˜´\n","            pairs = [(i, i+1) for i in range(len(word)-1)]\n","            # rankê°€ ê°€ì¥ ë‚®ì€ (ê°€ì¥ ë¨¼ì € ë°°ìš´ = ê°€ì¥ ë¹ˆë„ ë†’ì•˜ë˜) pair ì°¾ê¸°\n","            bigram = min(pairs,\n","                        key=lambda pair: self.bpe_ranks.get((word[pair[0]], word[pair[1]]), float('inf')))\n","\n","            if (word[bigram[0]], word[bigram[1]]) not in self.bpe_ranks:\n","                break  # ë” ì´ìƒ mergeí•  ê²Œ ì—†ìŒ\n","\n","            # 3. í•´ë‹¹ pairë¥¼ í•©ì¹¨\n","            a, b = bigram\n","            new_word = []\n","            i = 0\n","            while i < len(word):\n","                if i == a:\n","                    new_word.append(word[i] + word[i+1])\n","                    i += 2\n","                else:\n","                    new_word.append(word[i])\n","                    i += 1\n","            word = new_word\n","\n","        result = \" \".join(word)\n","        self._bpe_cache[token] = result\n","        return result\n","\n","    def encode(self, text):\n","        if not text: return []\n","        ids = []\n","        for m in self.pat.finditer(text):\n","            piece = m.group()\n","            token = \"\".join(self.byte_encoder[b] for b in piece.encode(\"utf-8\"))\n","            bpe_words = self._bpe(token).split()\n","            ids.extend(self.encoder.get(t, 50256) for t in bpe_words if t in self.encoder) # Safe get\n","        return ids\n","\n","    def decode(self, ids):\n","        clean_ids = [i for i in ids if isinstance(i, int)]\n","        text = \"\".join(self.decoder.get(i, \"\") for i in clean_ids)\n","        return bytearray(self.byte_decoder.get(c, 0) for c in text if c in self.byte_decoder).decode(\"utf-8\", errors=\"replace\")\n","\n","def expand_text(text: str, repeat: int = 100) -> str:\n","    return text * repeat\n","\n","# --- [ìˆ˜ì •ëœ ë©”ì¸ ì‹¤í–‰ ë¸”ë¡] ---\n","if __name__ == \"__main__\":\n","\n","    # 1. ì´ˆê¸°í™” (Initialization)\n","    print(\"ğŸš€ Initializing Tokenizer...\")\n","    # ìµœì´ˆ ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œ í›„ ì €ì¥, ì´í›„ ë¡œì»¬ ë¡œë“œ\n","    if not os.path.exists(\"./my_gpt2_resources\"):\n","        temp_tokenizer = ProductionGPT2Tokenizer(cache_size=5000)\n","        temp_tokenizer.save_pretrained(\"./my_gpt2_resources\")\n","\n","    # ë¡œì»¬ì—ì„œ ë¡œë“œ (ì†ë„ ì¸¡ì • ëŒ€ìƒ)\n","    prod_tokenizer = ProductionGPT2Tokenizer(\n","        vocab_path=\"./my_gpt2_resources/vocab.json\",\n","        merges_path=\"./my_gpt2_resources/merges.txt\",\n","        cache_size=100000\n","    )\n","\n","    # 2. ë°ì´í„° ì¤€ë¹„ (Data Preparation)\n","    print(\"\\nğŸŒ Fetching text from Tistory...\")\n","    url = \"https://uno-kim.tistory.com/431\"\n","\n","    if requests and bs4:\n","        try:\n","            # ì•ˆì „í•œ ì„ íƒì ì‚¬ìš© (ì´ì „ êµí›ˆ ë°˜ì˜)\n","            raw_text = bs4.BeautifulSoup(requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}).text, \"html.parser\").select_one(\".tt_article_useless_p_margin, .contents_style, .article_view, #content\").get_text(\"\\n\").strip()\n","            print(f\"ğŸ“„ Original Text Length: {len(raw_text):,} chars\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Crawling failed: {e}. Using dummy text.\")\n","            raw_text = \"Hello world. \" * 1000\n","    else:\n","        print(\"âš ï¸ Requests/BeautifulSoup not installed. Using dummy text.\")\n","        raw_text = \"Hello world. \" * 1000\n","\n","    # í…ìŠ¤íŠ¸ ë»¥íŠ€ê¸° (ë¶€í•˜ í…ŒìŠ¤íŠ¸ìš©)\n","    repeat_count = 100\n","    text = expand_text(raw_text, repeat_count)\n","    print(f\"ğŸ”¥ Expanded Text Length: {len(text):,} chars (x{repeat_count})\")\n","\n","    # 3. ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ (Benchmark Start)\n","    print(\"\\nâ± Starting Benchmark (Encode -> Decode)...\")\n","\n","    process = psutil.Process(os.getpid())\n","    mem_before = process.memory_info().rss / 1024**2  # MB ë‹¨ìœ„\n","\n","    start_time = time.time()\n","\n","    # --- í•µì‹¬ ì‘ì—… ---\n","    ids = prod_tokenizer.encode(text)\n","    decoded = prod_tokenizer.decode(ids)\n","    # ----------------\n","\n","    end_time = time.time()\n","    mem_after = process.memory_info().rss / 1024**2\n","\n","    # 4. ê²°ê³¼ ê²€ì¦ ë° ì¶œë ¥ (Validation & Report)\n","    is_valid = (text == decoded)\n","\n","    print(\"-\" * 40)\n","    print(f\"âœ… Round-trip Integrity: {'PASSED' if is_valid else 'FAILED'}\")\n","    print(f\"ğŸ“Š Total Tokens: {len(ids):,} tokens\")\n","    print(f\"ğŸ‘€ Token Sample (First 10): {ids[:10]} ...\")\n","    print(\"-\" * 40)\n","    print(f\"â±  Execution Time: {end_time - start_time:.4f} sec\")\n","    print(f\"ğŸ’¾ Memory Usage: {mem_after - mem_before:.2f} MB (Diff)\")\n","    print(f\"ğŸ’¾ Total Memory: {mem_after:.2f} MB\")\n","    print(\"-\" * 40)"]}]}